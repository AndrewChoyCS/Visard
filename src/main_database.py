from pipeline import Pipeline
from populate_pipeline import PopulatePipeline
from generate_synthetic_data import SyntheticDataGenerator

#topic 1 - Gradient Descent
g1 =  "Gradient descent is a method for unconstrained mathematical optimization. It is a first-order iterative algorithm for minimizing a differentiable multivariate function. The idea is to take repeated steps in the opposite direction of the gradient (or approximate gradient) of the function at the current point, because this is the direction of steepest descent. Conversely, stepping in the direction of the gradient will lead to a trajectory that maximizes that function; the procedure is then known as gradient ascent. It is particularly useful in machine learning for minimizing the cost or loss function." #Wikipedia (compare and contrast present)
g2 = r"""One thing to note, however, is that the techniques we used above can only be applied if we make some big assumptions. For the calculus approach, we assumed that the loss function was differentiable at all points and that we could algebraically solve for the zero points of the derivative; for the geometric approach, OLS *only* applies when using a linear model with MSE loss. What happens when we have more complex models with different, more complex loss functions? The techniques we've learned so far will not work, so we need a new optimization technique: **gradient descent**. Looking at the function across this domain, it is clear that the function's minimum value occurs around $\theta = 5.3$. Let's pretend for a moment that we *couldn't* see the full view of the cost function. How would we guess the value of $\theta$ that minimizes the function? Let's consider an arbitrary function. Our goal is to find the value of $x$ that minimizes this function.```def arbitrary(x): return (x**4 - 15*x**3 + 80*x**2 - 180*x + 144)/10 It turns out that the first derivative of the function can give us a clue. In the graph below, the function and its derivative are plotted, with points where the derivative is equal to 0 plotted in light green. > **BIG IDEA**: use an iterative algorithm to numerically compute the minimum of the loss.Looking at the function across this domain, it is clear that the function's minimum value occurs around $\theta = 5.3$. Let's pretend for a moment that we *couldn't* see the full view of the cost function. How would we guess the value of $\theta$ that minimizes the function?  It turns out that the first derivative of the function can give us a clue. In the graph below, the function and its derivative are plotted, with points where the derivative is equal to 0 plotted in light green.Say we make a guess for the minimizing value of $\theta$. Remember that we read plots from left to right, and assume that our starting $\theta$ value is to the left of the optimal $\hat{\theta}$. If the guess undershoots the true minimizing value – our guess for $\theta$ is lower than the value of the $\hat{\theta}$ that minimizes the function – the derivative will be **negative**. This means that if we increase $\theta$ (move further to the right), then we **can decrease** our loss function further. If this guess overshoots the true minimizing value, the derivative will be positive, implying the converse. We can use this pattern to help formulate our next guess for the optimal $\hat{\theta}$. Consider the case where we've undershot $\theta$ by guessing too low of a value. We'll want our next guess to be greater in value than our previous guess – that is, we want to shift our guess to the right. You can think of this as following the slope downhill to the function's minimum value.If we've overshot $\hat{\theta}$ by guessing too high of a value, we'll want our next guess to be lower in value – we want to shift our guess for $\hat{\theta}$ to the left.""" #Data 100 note (a lot of context + example function)
g3 = "Intuition for Gradient Descent Think of a large bowl like what you would eat cereal out of or store fruit in. This bowl is a plot of the cost function (f).A random position on the surface of the bowl is the cost of the current values of the coefficients (cost).The bottom of the bowl is the cost of the best set of coefficients, the minimum of the function. The goal is to continue to try different values for the coefficients, evaluate their cost and select new coefficients that have a slightly better (lower) costs. Repeating this process enough times will lead to the bottom of the bowl and you will know the values of the coefficients that result in the minimum cost." #Src: https://machinelearningmastery.com/gradient-descent-for-machine-learning/ (intuitive explanation)
g4 = "Gradient Descent Algorithm Gradient Descent Algorithm iteratively calculates the next point using gradient at the current position, scales it (by a learning rate) and subtracts obtained value from the current position (makes a step). It subtracts the value because we want to minimise the function (to maximise it would be adding). This process can be written as:p_{n+1} = p_n - η * ∇f(p_n) There’s an important parameter η which scales the gradient and thus controls the step size. In machine learning, it is called learning rate and have a strong influence on performance. The smaller learning rate the longer GD converges, or may reach maximum iteration before reaching the optimum point If learning rate is too big the algorithm may not converge to the optimal point (jump around) or even to diverge completely. In summary, Gradient Descent method’s steps are: 1-choose a starting point (initialisation), 2-calculate gradient at this point, 3-make a scaled step in the opposite direction to the gradient (objective: minimise), 4-repeat points 2 and 3 until one of the criteria is met: maximum number of iterations reached step size is smaller than the tolerance (due to scaling or a small gradient)." #Src: https://medium.com/data-science/gradient-descent-algorithm-a-deep-dive-cf04e8115f21 / step by step algorithm

#topic 2 - Convexity
c1 = "A function f : R n → R is convex if its domain is a convex set and for all x, y in its domain, and all λ ∈ [0, 1], we have f(λx + (1 − λ)y) ≤ λf(x) + (1 − λ)f(y).• In words, this means that if we take any two points x, y, then f evaluated at any convex combination of these two points should be no larger than the same convex combination of f(x) and f(y). Geometrically, the line segment connecting (x, f(x)) to (y, f(y)) must sit above the graph of f. If f is continuous, then to ensure convexity it is enough to check the definition with λ = 1/2 (or any other fixed λ ∈ (0, 1)). This is similar to the notion of midpoint convex sets that we saw earlier. We say that f is concave if −f is convex." #Src: https://www.princeton.edu/~aaa/Public/Teaching/ORF523/S16/ORF523_S16_Lec7_gh.pdf (recap, general description)
c2 = r""" The most basic duality theorem is that for any closed convex set C, and any point x0 /∈ C, there exists a hyperplane (equivalently, a functional x∗ ∈ X∗) that separates x0 from C. This may seem fairly obvious in R2 by drawing a picture, but the result holds in arbitrary (in fact, infinite!) dimensions. The result is often referred to as the geometric Hahn–Banach theorem, of which there are many different but related variations; we state a few below. Theorem 3.5 (The Separation Theorem).Let C ⊂ X be a closed convex set, and x0 ∈ X \ C. There exists a nonzero x∗ ∈ X∗ and δ > 0 such that ⟨x∗, x0⟩ + δ ≤ ⟨x∗, x⟩, ∀x ∈ C. In other words, x∗ (strictly) separates x0 from C.""" #Src: EECS 127 Sp25 handout by T.Courtade (The Separation Theorem)
c3 = "A convex function is a continuous function whose value at the midpoint of every interval in its domain does not exceed the arithmetic mean of its values at the ends of the interval.If f(x) has a second derivative in [a,b], then a necessary and sufficient condition for it to be convex on that interval is that the second derivative f^('')(x)>=0 for all x in [a,b]." #Src: https://mathworld.wolfram.com/ConvexFunction.html (General Definiton + extra context)
c4 = "To simply things, think of convex sets as shapes where any line joining 2 points in this set is never outside the set. This is called a convex set.Consider the graph of a function f. An epigraph is a set of points lying on or above the function’s graph.A function f is said to be a convex function if its epigraph is a convex set. This means that every line segment drawn on this graph is always equal to or above the function graph. " #Src:https://towardsdatascience.com/understand-convexity-in-optimization-db87653bf920/ (Sequence of definitions)

#topic 3 - Least Squares
l1 = r"""Let  𝐴 be an  𝑚×𝑛 matrix and let  𝑏 be a vector in  ℝ𝑚. A least-squares solution of the matrix equation  𝐴𝑥=𝑏 is a vector  𝑥̂  in  ℝ𝑛 such that dist(𝑏,𝐴𝑥̂ )≤dist(𝑏,𝐴𝑥) for all other vectors  𝑥 in  ℝ𝑛.Col(𝐴) is the set of all vectors of the form 𝐴𝑥. Recall that  dist(𝑣,𝑤)=‖𝑣−𝑤‖ is the distance, Definition 6.1.2 in Section 6.1, between the vectors  𝑣 and  𝑤. The term “least squares” comes from the fact that  dist(𝑏,𝐴𝑥)=‖𝑏−𝐴𝑥̂ ‖ is the square root of the sum of the squares of the entries of the vector  𝑏−𝐴𝑥̂  . So a least-squares solution minimizes the sum of the squares of the differences between the entries of  𝐴𝑥̂  and  𝑏 . In other words, a least-squares solution solves the equation  𝐴𝑥=𝑏 as closely as possible, in the sense that the sum of the squares of the difference  𝑏−𝐴𝑥 is minimized.Hence, the closest vector, Note 6.3.1 in Section 6.3, of the form 𝐴𝑥 to 𝑏 is the orthogonal projection of 𝑏 onto Col(𝐴). This is denoted 𝑏Col(𝐴)""" #definition + lots of letter notation src: https://math.libretexts.org/Bookshelves/Linear_Algebra/Interactive_Linear_Algebra_(Margalit_and_Rabinoff)/06%3A_Orthogonality/6.5%3A_The_Method_of_Least_Squares
l2 = r"""So the situation we’re in is that ¯b is not in Col(A) and we wish to find ˆx so that Axˆ, which is in Col(A), is as close as possible to ¯b.This picture suggests that we can obtain a solution by projecting ¯b onto Col(A) to get PrCol(A) ¯b and then finding ˆx to solve the equation: Axˆ = PrCol(A) ¯b Assuming this is correct, the problem with this approach, practically, is that calculating PrCol(A) ¯b requires having an orthogonal basis for Col(A) and this is procedurally intense especially when A is large. So what we’ll do is find a sneaky way to find ˆx a different way. Just to be clear, we will solve this equation, but we won’t solve it by finding PrCol(A) ¯b.""" #general description src: https://www.math.umd.edu/~immortal/MATH401/book/ch_least_squares.pdf
l3 = r"""Let’s look at the method of least squares from another perspective. Imagine that you’ve plotted some data using a scatterplot, and that you fit a line for the mean of Y through the data. Let’s lock this line in place, and attach springs between the data points and the line. Some of the data points are further from the mean line, so these springs are stretched more than others. The springs that are stretched the furthest exert the greatest force on the line. What if we unlock this mean line, and let it rotate freely around the mean of Y? The forces on the springs balance, rotating the line. The line rotates until the overall force on the line is minimized. The are some cool physics at play, involving the relationship between force and the energy needed to pull a spring a given distance. It turns out that minimizing the overall energy in the springs is equivalent to fitting a regression line using the method of least squares.""" #intuitive explanation src: https://www.jmp.com/en/statistics-knowledge-portal/what-is-regression/the-method-of-least-squares#:~:text=So%20how%20do%20we%20measure,sum%20of%20the%20squared%20errors.&text=This%20method%2C%20the%20method%20of,sum%20of%20the%20squared%20errors.
l4 = r"""Suppose someone hands you a stack of N vectors, {~x1, . . . ~xN }, each of dimension d, and an scalar observation associated with each one, {y1, . . . , yN }. In other words, the data now come in pairs (~xi , yi), where each pair has one vector (known as the input, the regressor, or the predictor) and a scalar (known as the output or dependent variable). Suppose we would like to estimate a linear function that allows us to predict y from ~x as well as possible: in other words, we’d like a weight vector ~w such that yi ≈ ~w >~xi . Specifically, we’d like to minimize the squared prediction error, so we’d like to find the ~w that minimizes squared error = X N i=1 (yi − ~xi · ~w) 2 (1) We’re going to write this as a vector equation to make it easier to derive the solution. Let Y be a vector composed of the stacked observations {yi}, and let X be the vector whose rows are the vectors {~xi} (which is known as the design matrix): Y =    y1 . . . yN    X =    — ~x1 — . . . — ~xN —    Then we can rewrite the squared error given above as the squared vector norm of the residual error between Y and X ~w: squared error = ||Y − X ~w||2 (2) The solution (stated here without proof): the vector that minimizes the above squared error (which we equip with a hat ˆ~w to denote the fact that it is an estimate recovered from data) is: ~w = (X>X) −1 (X>Y ). 2 Derivation 1: using orthogonality I will provide two derivations of the above formula, though we will only have time to discuss the first one (which is a little bit easier) in class. It has the added advantage that it gives us some insight into the geometry of the problem. 1 Let’s think about the design matrix X in terms of its d columns instead of its N rows. Let {Xj} denote the j 0 th column, i.e., X =    X1 · · · Xd    (3) The columns of X span a d-dimensional subspace within the larger N-dimensional vector space that contains the vector Y . Generally Y does not lie exactly within this subspace. Least squares regression is therefore trying to find the linear combination of these vectors, X ~w, that gets as close to possible to Y . What we know about the optimal linear combination is that it corresponds to dropping a line down from Y to the subspace spanned by {X1, . . . XD} at a right angle. In other words, the error vector (Y − X ~w) (also known as the residual error) should be orthogonal to every column of X: (Y − X ~w) · Xj = 0, (4) for all columns j = 1 up to j = d. Written as a matrix equation this means: (Y − X ~w) >X = ~0 (5) where ~0 is d-component vector of zeros. We should quickly be able to see that solving this for ~w gives us the solution we were looking for: X>(Y − X ~w) = X>Y − X>X ~w = 0 (6) =⇒ (X>X) ~w = X>Y (7) =⇒ ~w = (X>X) −1X>Y. (8) So to summarize: the requirement that the residual errors Y − X ~w be orthogonal to the columns of X was all we needed to derive the optimal weight vector ~w.  """ #derivation src: https://pillowlab.princeton.edu/teaching/statneuro2018/slides/notes03b_LeastSquaresRegression.pdf

#topic 4 - Coordinate Descent
co1 = r"""Recall from Chapter 6 the general idea of descent-based methods for solving optimization problems. These methods are based on the process of starting with some initial guess ⃗x(0) ∈ R n, then generating a sequence of refined guesses ⃗x(1), ⃗x(2), ⃗x(3) , . . . using the general update rule ⃗x(t+1) = ⃗x(t) + η⃗v(t) (10.1) for some search direction ⃗v(t) and step size η. In Chapter 6 we covered the gradient descent method, which uses the gradient of the function as the search direction. In this chapter we will revisit descent-based optimization methods and introduce alternative update rules. In this section we will introduce coordinate descent, a class of descent-based algorithms that finds a minimizer of multivariate functions by iteratively minimizing it along one direction at a time. Consider the unconstrained convex optimization problem p ⋆ = min ⃗x∈Rn f(⃗x), (10.2) with the optimization variable ⃗x =       x1 x2 . . . xn       . (10.3) Before we introduce the algorithm, we introduce some notation. For indices i and j, we introduce the notation ⃗xi:j = (xi , xi+1, . . . , xj−1, xj ) ∈ R j−i+1 to be the entries of ⃗x between indices i and j (inclusive on both ends). Given an initial guess ⃗x(0), for t ≥ 0 the coordinate descent algorithm updates the iterate ⃗x(t) by sequentially minimizing the function f(⃗x) with respect to each coordinate, namely using the update rule x (t+1) i ∈ argmin xi∈R f(⃗x(t+1) 1:i−1 , xi , ⃗x(t) i+1:n ). (10.4) 141 EECS 127/227AT Course Reader 2023-05-09 00:11:08-07:00 Namely, we perform the update x (t+1) 1 ∈ argmin x1∈R f(x1, ⃗x(t) 2:n ) (10.5) x (t+1) 2 ∈ argmin x2∈R f(x (t+1) 1 , x2, ⃗x(t) 3:n ) (10.6) . . . . . . (10.7) x (t+1) n ∈ argmin xn∈R f(⃗x(t+1) 1:n−1 , xn). (10.8) This is a sequential process since after finding the minimizer along the i th coordinate (i.e. x (t+1) i ) we use its values for minimizing subsequent coordinates. Also we note that the order of the coordinates is arbitrary. We formalize this update in the following algorithm. Algorithm 6 CoordinateDescent 1: function CoordinateDescent(f, ⃗x(0), T) 2: for t = 0, 1, . . . , T1 do 3: for i = 1, . . . , N do 4: x (t+1) i ← argminxi∈R f(x (t+1) 1:i−1 , xi , x (t) i+1:n ). 5: end for 6: end for 7: return ⃗xT 8: end function The algorithm breaks down the difficult multivariate optimization problem into a sequence of simpler univariate optimization problems. We first want to discuss the issue of well-posedness of the algorithm. We know that any of the argmins used may not exist, in which case the algorithm is not well-defined, and so we cannot even think about its behavior or convergence. Nevertheless, in a large class of problems which have many different characterizations, the argmins are well-defined. We say in this case that the coordinate descent algorithm is well-posed. We now want to address the question of convergence. It is not obvious that minimizing the function f(⃗x) can be achieved by minimizing along each direction separately. In fact, the algorithm is not guaranteed to converge to an optimal solution for general convex functions. However, under some additional assumptions on the function, we can guarantee convergence. To build an intuition for what additional assumptions are needed we consider the following question. Let f(⃗x) be a convex differentiable function. Suppose that x ⋆ i ∈ argminxi∈R f(x ⋆ 1:i−1 , xi , x⋆ i+1:n ) for all i. Can we conclude that ⃗x⋆ is a global minimizer of f(⃗x)? The answer to this question is yes. We can prove this by recalling the first order optimality condition for unconstrained convex functions and the definition of partial derivatives. If ⃗x⋆ is a minimizer of f(⃗x) along the direction ⃗ei then we have ∂f ∂xi (⃗x⋆ ) = 0. (10.9) If this is true for all i then ∇f(⃗x⋆ ) = ⃗0, implying that ⃗x⋆ is a global minimizer for f. This discussion forms a proof of the following theorem, which is Theorem 12.4 in [2]. © UCB EECS 127/227AT, Spring 2023. All Rights Reserved. This may not be publicly shared without explicit permission. 142 EECS 127/227AT Course Reader 2023-05-09 00:11:08-07:00 Theorem 179 (Convergence of Coordinate Descent for Differentiable Convex Functions) Let f : R n → R be a differentiable convex function which is separately strictly convex in each argument. That is, suppose that for each i, and each fixed ⃗x1:i−1 and ⃗xi+1:n, the function xi 7→ f(⃗x1:i−1, xi , ⃗xi+1:n) is strictly convex. If the coordinate descent algorithm is well-posed, and the unconstrained minimization problem min ⃗x∈Rn f(⃗x) (10.10) has a solution, then the sequence of iterates ⃗x(0), ⃗x(1) , . . . generated by the coordinate descent algorithm converges to an optimal solution to (10.10). The coordinate descent algorithm may not converge to an optimal solution for general non-differentiable functions, even if they are convex. However, we can still prove that coordinate descent converges for a special class of functions of the form f(⃗x) = g(⃗x) +Xn i=1 hi(xi) (10.11) where g : R n → R is convex and differentiable, and each hi : R → R is convex (but not necessarily differentiable). This form includes various ℓ 1 regularization problems (such as LASSO regression) which have a separable nondifferentiable component. The provable convergence of coordinate descent algorithm makes it an attractive choice for this class of problems"""#eecs127 reader - lots of context
co2 = r"""Coordinate descent algorithms solve optimization problems by successively performing approximate minimization along coordinate directions or coordinate hyperplanes.They are iterative methods in which each iterate is obtained by fixing most components of the variable vector x at their values from the current iteration, and approximately minimizing the objective with respect to the remaining components. Each such subproblem is a lowerdimensional (even scalar) minimization problem, and thus can typically be solved more easily than the full problem.""" #general intro src: https://arxiv.org/pdf/1502.04759
co3 = r"""If we take a simple quadratic function we can take a detailed look at how coordinate descent works. Let’s use the function f ( x , y ) = x 2 + y 2 + x y . Let’s take as our initial point  ( − 1 , − 1 ) and begin our minimization along the x dimension. We can draw a transect at the y = − 1 level (thus holding y constant) and attempt to find the minimum along that transect. Because f is a quadratic function, the one-dimensional function induced by holding y = − 1 is also a quadratic.We can see that after two iterations we are quite a bit closer to the minimum. But we still have a ways to go, given that we can only move along the coordinate axis directions.""" #concrete example src:https://bookdown.org/rdpeng/advstatcomp/coordinate-descent.html
co4 = r"""Given convex, differentiable f : R n → R, if we are at a point x such that f(x) is minimized along each coordinate axis, have we found a global minimizer. This suggests that for f(x) = g(x) + Pn i=1 hi(xi) (with g convex, differentiable and each hi convex) we can use coordinate descent to find a minimizer: start with some initial guess x (0), and repeat x (k) 1 ∈ argmin x1 f x1, x (k−1) 2 , x (k−1) 3 , . . . x(k−1) n x (k) 2 ∈ argmin x2 f x (k) 1 , x2, x (k−1) 3 , . . . x(k−1) n  x (k) 3 ∈ argmin x2 f x (k) 1 , x (k) 2 , x3, . . . x(k−1) n  . . . x (k) n ∈ argmin x2 f x (k) 1 , x (k) 2 , x (k) 3 , . . . xn  for k = 1, 2, 3, . . . (note: after we solve for x (k) i , we use its new value from then """ #description + step by step https://www.stat.cmu.edu/~ryantibs/convexopt-F13/lectures/24-coord-desc.pdf

#topic 5 - Maxima and Minima of functions
d1=r"""The inflection point(s) of a function is the x-value(s) at which the second derivative is zero or undefined and the function is changing concavity. You can tell that the function changes concavity if the second derivative changes signs. The inflection points are point 1 and point 2 because the second derivative of the function equals zero or is undefined at those x-values, and the sign of the second derivative changes signs:  At point 1 the second derivative changes signs from negative to positive, which means the function changes concavity from concave down to concave up.  At point 2 the second derivative changes signs from positive to negative, which means the function changes concavity from concave up to concave down.  Although the second derivative is undefined at , it is not an inflection point because the second derivative does not change signs, it remains concave up.""" #infelction point src:http://www.leilehuacalculus.weebly.com/uploads/4/9/1/4/4914438/calc_lecture_notes_8-2aa.pdf
d2="We say that f ( x ) has an absolute (or global) maximum at x = c if f ( x ) ≤ f ( c ) for every x in the domain we are working on. We say that f ( x ) has a relative (or local) maximum at x = c if f ( x ) ≤ f ( c ) for every x in some open interval around x = c . We say that f ( x ) has an absolute (or global) minimum at x = c if f ( x ) ≥ f ( c ) for every x in the domain we are working on. We say that f ( x ) has a relative (or local) minimum at x = c if f ( x ) ≥ f ( c ) for every x in some open interval around x = c ." #local vs global max/min src: https://tutorial.math.lamar.edu/classes/calcI/minmaxvalues.aspx
d3="The first order derivative test as the name suggests it uses first order derivative to find maxima and minima. The first order derivative gives the slope of the function. Let f be a continuous function at critical point c on the open interval l such that f'(c) = 0 then, we will check the nature of the curve. Below are some conditions after checking the nature of the curve, and x increases towards c i.e., the critical point. If the sign of f'(x) changes from positive to negative, then f(c) is the maximum value and c is the point of local maxima. If the sign of f'(x) changes from negative to positive, then f(c) is the minimum value and c is the point of local minima. If the sign of f'(x) neither changes from positive to negative nor from negative to positive, then c is called the point of inflection i.e., neither maxima nor minima." #first order derivative test src:https://www.geeksforgeeks.org/maxima-and-minima/
d4="Example: A ball is thrown in the air. Its height at any time t is given by: h = 3 + 14t − 5t2 What is its maximum height? Using derivatives we can find the slope of that function: h' = 0 + 14 − 5(2t) = 14 − 10t (See below this example for how we found that derivative.) quadratic graph Now find when the slope is zero: 14 − 10t = 0 10t = 14 t = 14 / 10 = 1.4 The slope is zero at t = 1.4 seconds And the height at that time is: h = 3 + 14×1.4 − 5×1.42 h = 3 + 19.6 − 9.8 = 12.8 And so: The maximum height is 12.8 m (at t = 1.4 s)" #concrete example src:https://www.mathsisfun.com/calculus/maxima-minima.html

gen=SyntheticDataGenerator(4)

data_dict = {
    # "Gradient Descent":                     [g1, g2, g3, g4],
    "Convexity":                            [c1, c2, c3, c4],
    "Least Squares":                        [l1, l2, l3, l4],
    "Coordinate Descent":                   [co1, co2, co3, co4],
    "Critical and Inflection points, Maxima and Minima of functions": [d1, d2, d3, d4]
}# data_dict={"Gradient Descent":[g1,g2,g3,g4],"Convexity":[c1,c2,c3,c4], }

for data_topic, data_list in data_dict.items():
    original=list(data_list)
    for text in original:
        paraphrases = gen._generate_data2(text)
        data_list.extend(paraphrases)
    PopulatePipeline(data_list, data_topic)
        

# pipe = Pipeline(data, "Gradient Descent")
# pipe = PopulatePipeline(data, "Gradient Descent")ƒ