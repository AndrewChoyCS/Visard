{
    "data": "Suppose someone hands you a stack of N vectors, {~x1, . . . ~xN }, each of dimension d, and an scalar observation associated with each one, {y1, . . . , yN }. In other words, the data now come in pairs (~xi , yi), where each pair has one vector (known as the input, the regressor, or the predictor) and a scalar (known as the output or dependent variable). Suppose we would like to estimate a linear function that allows us to predict y from ~x as well as possible: in other words, we\u2019d like a weight vector ~w such that yi \u2248 ~w >~xi . Specifically, we\u2019d like to minimize the squared prediction error, so we\u2019d like to find the ~w that minimizes squared error = X N i=1 (yi \u2212 ~xi \u00b7 ~w) 2 (1) We\u2019re going to write this as a vector equation to make it easier to derive the solution. Let Y be a vector composed of the stacked observations {yi}, and let X be the vector whose rows are the vectors {~xi} (which is known as the design matrix): Y = \uf8ee \uf8ef \uf8f0 y1 . . . yN \uf8f9 \uf8fa \uf8fb X = \uf8ee \uf8ef \uf8f0 \u2014 ~x1 \u2014 . . . \u2014 ~xN \u2014 \uf8f9 \uf8fa \uf8fb Then we can rewrite the squared error given above as the squared vector norm of the residual error between Y and X ~w: squared error = ||Y \u2212 X ~w||2 (2) The solution (stated here without proof): the vector that minimizes the above squared error (which we equip with a hat \u02c6~w to denote the fact that it is an estimate recovered from data) is: ~w = (X>X) \u22121 (X>Y ). 2 Derivation 1: using orthogonality I will provide two derivations of the above formula, though we will only have time to discuss the first one (which is a little bit easier) in class. It has the added advantage that it gives us some insight into the geometry of the problem. 1 Let\u2019s think about the design matrix X in terms of its d columns instead of its N rows. Let {Xj} denote the j 0 th column, i.e., X = \uf8ee \uf8ef \uf8f0 X1 \u00b7 \u00b7 \u00b7 Xd \uf8f9 \uf8fa \uf8fb (3) The columns of X span a d-dimensional subspace within the larger N-dimensional vector space that contains the vector Y . Generally Y does not lie exactly within this subspace. Least squares regression is therefore trying to find the linear combination of these vectors, X ~w, that gets as close to possible to Y . What we know about the optimal linear combination is that it corresponds to dropping a line down from Y to the subspace spanned by {X1, . . . XD} at a right angle. In other words, the error vector (Y \u2212 X ~w) (also known as the residual error) should be orthogonal to every column of X: (Y \u2212 X ~w) \u00b7 Xj = 0, (4) for all columns j = 1 up to j = d. Written as a matrix equation this means: (Y \u2212 X ~w) >X = ~0 (5) where ~0 is d-component vector of zeros. We should quickly be able to see that solving this for ~w gives us the solution we were looking for: X>(Y \u2212 X ~w) = X>Y \u2212 X>X ~w = 0 (6) =\u21d2 (X>X) ~w = X>Y (7) =\u21d2 ~w = (X>X) \u22121X>Y. (8) So to summarize: the requirement that the residual errors Y \u2212 X ~w be orthogonal to the columns of X was all we needed to derive the optimal weight vector ~w.  ",
    "goal": "**Create a visualization to explain the relationship between the vectors in the design matrix X, the observations Y, and the optimal weight vector \\(\\hat{w}\\) in Least Squares Regression.** \n\nThe goal of this visualization is to clarify how the optimal weight vector minimizes the residual error in a linear regression model. Specifically, it should illustrate the geometric relationship whereby the vector that represents the prediction \\(X\\hat{w}\\) approaches \\(Y\\) at a right angle, highlighting the orthogonality condition that the residual error \\(Y - X\\hat{w}\\) must satisfy with respect to the columns of \\(X\\). \n\nUsing a combination of a 3D geometric plot and a 2D projection, the visualization can depict the subspace spanned by the columns of \\(X\\) and the point corresponding to \\(Y\\), along with a line representing the residual error. This style allows learners to visually grasp the need for \\(Y\\) to be projected orthogonally onto the subspace of \\(X\\). \n\nBy engaging with this visual, learners will deepen their understanding of how the Least Squares method effectively finds the best linear approximation for predicting \\(Y\\) using \\(X\\), as well as reinforcing the critical concept of orthogonality in minimizing squared differences.\n\n**Objective:** Students will gain an in-depth understanding of how the geometry of vector spaces relates to the Least Squares method and how orthogonal projections facilitate the minimization of residuals in regression analysis.",
    "code": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Create data for the design matrix X and observations Y\nnp.random.seed(0)\nX = np.random.rand(3, 2)  # 3 observations on 2 features\nw_true = np.array([2, 3])\nY = X @ w_true + np.random.normal(0, 0.1, 3)  # Y with small noise\n\n# Calculate the optimal weight vector using Least Squares\nw_hat = np.linalg.inv(X.T @ X) @ X.T @ Y\nY_pred = X @ w_hat\n\n# Compute residuals\nresiduals = Y - Y_pred\n\n# Set up the 3D plot\nfig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(111, projection='3d')\n\n# Plot the original observations\nax.scatter(X[:, 0], X[:, 1], Y, color='blue', s=100, label='Observations Y', alpha=0.6)\n\n# Plot the predicted points in the subspace\nax.scatter(X[:, 0], X[:, 1], Y_pred, color='orange', s=100, label='Predictions $X\\\\hat{w}$', alpha=0.6)\n\n# Plot the residuals\nfor i in range(len(Y)):\n    ax.plot([X[i, 0], X[i, 0]], [X[i, 1], X[i, 1]], [Y[i], Y_pred[i]], color='red', linestyle='--', label='Residuals' if i == 0 else \"\")\n\n# Indicate orthogonality\nfor j in range(X.shape[1]):\n    ax.plot([Y_pred[0], Y_pred[0] + 0.1 * X[0, j]], [Y_pred[1], Y_pred[1] + 0.1 * X[1, j]], \n            [Y_pred[2], Y_pred[2]], color='green', label='Projection on X.Columns' if j == 0 else \"\")\n    \n# Set axis labels\nax.set_xlabel('Feature 1')\nax.set_ylabel('Feature 2')\nax.set_zlabel('Response Y')\nax.set_title('Least Squares Regression - Geometric Interpretation')\nax.legend()\n\n# 2D Projection\nfig2 = plt.figure(figsize=(8, 8))\nplt.scatter(X[:, 0], Y, color='blue', label='Observations Y', alpha=0.6)\nplt.scatter(X[:, 0], Y_pred, color='orange', label='Predictions $X\\\\hat{w}$', alpha=0.6)\nplt.plot(X[:, 0], Y - Y_pred, color='red', linestyle='--', label='Residuals')\nplt.xlabel('Feature 1')\nplt.ylabel('Response Y')\nplt.title('Least Squares Regression - 2D Projection')\nplt.legend()",
    "metrics": {
        "run_start_time_iso": "2025-04-27T20:22:25.020436",
        "run_end_time_iso": "2025-04-27T20:24:22.852446",
        "topic": "Least Squares",
        "pipeline_success": true,
        "end_to_end_latency_seconds": 117.83,
        "total_api_calls": 4,
        "api_calls_per_agent": {
            "simple_query_agent": 1,
            "visualization_code_generator_agent": 1,
            "goal_alignment_judge_agent": 1,
            "visual_clarity_judge_agent": 1
        },
        "initial_code_generation_success": true,
        "code_execution_attempts": 1,
        "debugging_failed": false,
        "judge_feedback_loops": 0,
        "initial_goal_alignment_score": 4,
        "initial_visual_clarity_score": 4,
        "goal_alignment_scores": [
            4
        ],
        "visual_clarity_scores": [
            4
        ],
        "final_code_generated": true,
        "error_message": null,
        "input_data_snippet": "Suppose someone hands you a stack of N vectors, {~x1, . . . ~xN }, each of dimension d, and an scalar observation associated with each one, {y1, . . . , yN }. In other words, the data now come in pairs (~xi , yi), where each pair has one vector (known as the input, the regressor, or the predictor) and a scalar (known as the output or dependent variable). Suppose we would like to estimate a linear function that allows us to predict y from ~x as well as possible: in other words, we\u2019d like a weight vector ~w such that yi \u2248 ~w >~xi . Specifically, we\u2019d like to minimize the squared prediction error, so we\u2019d like to find the ~w that minimizes squared error = X N i=1 (yi \u2212 ~xi \u00b7 ~w) 2 (1) We\u2019re going to write this as a vector equation to make it easier to derive the solution. Let Y be a vector composed of the stacked observations {yi}, and let X be the vector whose rows are the vectors {~xi} (which is known as the design matrix): Y = \uf8ee \uf8ef \uf8f0 y1 . . . yN \uf8f9 \uf8fa \uf8fb X = \uf8ee \uf8ef \uf8f0 \u2014 ~x1 \u2014 . . . \u2014 ~xN \u2014 \uf8f9 \uf8fa \uf8fb Then we can rewrite the squared error given above as the squared vector norm of the residual error between Y and X ~w: squared error = ||Y \u2212 X ~w||2 (2) The solution (stated here without proof): the vector that minimizes the above squared error (which we equip with a hat \u02c6~w to denote the fact that it is an estimate recovered from data) is: ~w = (X>X) \u22121 (X>Y ). 2 Derivation 1: using orthogonality I will provide two derivations of the above formula, though we will only have time to discuss the first one (which is a little bit easier) in class. It has the added advantage that it gives us some insight into the geometry of the problem. 1 Let\u2019s think about the design matrix X in terms of its d columns instead of its N rows. Let {Xj} denote the j 0 th column, i.e., X = \uf8ee \uf8ef \uf8f0 X1 \u00b7 \u00b7 \u00b7 Xd \uf8f9 \uf8fa \uf8fb (3) The columns of X span a d-dimensional subspace within the larger N-dimensional vector space that contains the vector Y . Generally Y does not lie exactly within this subspace. Least squares regression is therefore trying to find the linear combination of these vectors, X ~w, that gets as close to possible to Y . What we know about the optimal linear combination is that it corresponds to dropping a line down from Y to the subspace spanned by {X1, . . . XD} at a right angle. In other words, the error vector (Y \u2212 X ~w) (also known as the residual error) should be orthogonal to every column of X: (Y \u2212 X ~w) \u00b7 Xj = 0, (4) for all columns j = 1 up to j = d. Written as a matrix equation this means: (Y \u2212 X ~w) >X = ~0 (5) where ~0 is d-component vector of zeros. We should quickly be able to see that solving this for ~w gives us the solution we were looking for: X>(Y \u2212 X ~w) = X>Y \u2212 X>X ~w = 0 (6) =\u21d2 (X>X) ~w = X>Y (7) =\u21d2 ~w = (X>X) \u22121X>Y. (8) So to summarize: the requirement that the residual errors Y \u2212 X ~w be orthogonal to the columns of X was all we needed to derive the optimal weight vector ~w.  "
    }
}