{
    "data": "Gradient descent is an approach utilized for unconstrained mathematical optimization. This first-order iterative method aims to minimize a multivariate function that is differentiable. The technique involves taking consistent steps in the reverse direction of the gradient\u2014or an estimated gradient\u2014at the current position, as this points towards the pathway of the greatest decline. On the other hand, making progress in the direction of the gradient will yield a path that maximizes the function, a method referred to as gradient ascent. This strategy is especially valuable in machine learning for reducing the cost or loss function.",
    "goal": "**Core Principle:** The process and implications of gradient descent and gradient ascent in optimizing functions.\n\n---\n\n**Query:** Create a visualization to explain the concepts of gradient descent and gradient ascent, highlighting their roles in optimizing multivariate functions. The visualization should represent both methods as paths on a 3D surface plot of a differentiable function, where the gradient indicates the direction of steepest ascent or descent. The goal is to clarify the relationship between the gradient, the step taken, and the optimization outcome, emphasizing how the direction of movement impacts function minimization and maximization.\n\nIn the visualization, employ a combination of a 3D surface diagram along with arrows indicating the gradient direction, distinct paths for both gradient descent and ascent, and labeled start and end points to illustrate the iterative nature of the methods. Integrating clear annotations and color coding (such as red for descent and green for ascent) will help differentiate the two processes while emphasizing their goals in the context of function optimization.\n\n**Learning Objective:** Students will develop a deep understanding of how gradient descent minimizes functions and how gradient ascent maximizes them, recognizing the significance of the gradient direction in influencing optimization strategies in machine learning. This visualization will enhance their comprehension of cause-and-effect relationships through a clear and engaging graphical representation of the processes involved.",
    "code": "```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Generate a grid of values\nX = np.linspace(-3, 3, 100)\nY = np.linspace(-3, 3, 100)\nX, Y = np.meshgrid(X, Y)\nZ = np.sin(np.sqrt(X**2 + Y**2))\n\n# Create a 3D surface plot\nfig = plt.figure(figsize=(10, 8))\nax = fig.add_subplot(111, projection='3d')\nax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8)\n\n# Gradient descent path\nstart_point_desc = np.array([2, 2])\nend_point_desc = np.array([-2, -2])\npath_desc = [start_point_desc]\n\nfor _ in range(20):\n    gradient = np.array([np.cos(np.sqrt(path_desc[-1][0]**2 + path_desc[-1][1]**2)) * path_desc[-1][0] / (np.sqrt(path_desc[-1][0]**2 + path_desc[-1][1]**2) + 1e-8),\n                         np.cos(np.sqrt(path_desc[-1][0]**2 + path_desc[-1][1]**2)) * path_desc[-1][1] / (np.sqrt(path_desc[-1][0]**2 + path_desc[-1][1]**2) + 1e-8)])\n    path_desc.append(path_desc[-1] - 0.1 * gradient)\n\npath_desc = np.array(path_desc)\nax.plot(path_desc[:, 0], path_desc[:, 1], np.sin(np.sqrt(path_desc[:, 0]**2 + path_desc[:, 1]**2)), color='red', linewidth=2, label='Gradient Descent')\n\n# Gradient ascent path\nstart_point_ascent = np.array([-2, -2])\nend_point_ascent = np.array([2, 2])\npath_ascent = [start_point_ascent]\n\nfor _ in range(20):\n    gradient = np.array([np.cos(np.sqrt(path_ascent[-1][0]**2 + path_ascent[-1][1]**2)) * path_ascent[-1][0] / (np.sqrt(path_ascent[-1][0]**2 + path_ascent[-1][1]**2) + 1e-8),\n                         np.cos(np.sqrt(path_ascent[-1][0]**2 + path_ascent[-1][1]**2)) * path_ascent[-1][1] / (np.sqrt(path_ascent[-1][0]**2 + path_ascent[-1][1]**2) + 1e-8)])\n    path_ascent.append(path_ascent[-1] + 0.1 * gradient)\n\npath_ascent = np.array(path_ascent)\nax.plot(path_ascent[:, 0], path_ascent[:, 1], np.sin(np.sqrt(path_ascent[:, 0]**2 + path_ascent[:, 1]**2)), color='green', linewidth=2, label='Gradient Ascent')\n\n# Annotations and labels\nax.scatter(start_point_desc[0], start_point_desc[1], np.sin(np.sqrt(start_point_desc[0]**2 + start_point_desc[1]**2)), color='red', s=100, label='Start Descend')\nax.scatter(end_point_desc[0], end_point_desc[1], np.sin(np.sqrt(end_point_desc[0]**2 + end_point_desc[1]**2)), color='red', s=100, label='End Descend')\n\nax.scatter(start_point_ascent[0], start_point_ascent[1], np.sin(np.sqrt(start_point_ascent[0]**2 + start_point_ascent[1]**2)), color='green', s=100, label='Start Ascent')\nax.scatter(end_point_ascent[0], end_point_ascent[1], np.sin(np.sqrt(end_point_ascent[0]**2 + end_point_ascent[1]**2)), color='green', s=100, label='End Ascent')\n\nax.set_xlabel('X-axis')\nax.set_ylabel('Y-axis')\nax.set_zlabel('Z-axis (f(X, Y))')\nax.set_title('Gradient Descent vs Gradient Ascent on a 3D Surface')\nax.legend()\n```"
}