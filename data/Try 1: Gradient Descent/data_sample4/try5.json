{
    "data": "Gradient Descent Algorithm Gradient Descent Algorithm iteratively calculates the next point using gradient at the current position, scales it (by a learning rate) and subtracts obtained value from the current position (makes a step). It subtracts the value because we want to minimise the function (to maximise it would be adding). This process can be written as:p_{n+1} = p_n - \u03b7 * \u2207f(p_n) There\u2019s an important parameter \u03b7 which scales the gradient and thus controls the step size. In machine learning, it is called learning rate and have a strong influence on performance. The smaller learning rate the longer GD converges, or may reach maximum iteration before reaching the optimum point If learning rate is too big the algorithm may not converge to the optimal point (jump around) or even to diverge completely. In summary, Gradient Descent method\u2019s steps are: 1-choose a starting point (initialisation), 2-calculate gradient at this point, 3-make a scaled step in the opposite direction to the gradient (objective: minimise), 4-repeat points 2 and 3 until one of the criteria is met: maximum number of iterations reached step size is smaller than the tolerance (due to scaling or a small gradient).",
    "goal": "**Create a visualization to explain the Gradient Descent Algorithm's iterative process, particularly the relationship between the learning rate (\u03b7) and the convergence behavior of the algorithm.** \n\nThe goal of this visualization is to clarify how different values of the learning rate impact the convergence of the algorithm, highlighting the balance between step size and the risk of overshooting the optimal point. It should showcase a sequence of steps in the iterative process, illustrating how the initial point moves along the gradient towards the minimum. This visualization could take the form of an animated flowchart or infographic, displaying varying scenarios: a small learning rate resulting in slow convergence, an optimal learning rate achieving smooth convergence, and a large learning rate leading to divergence. \n\nBy visually representing these relationships and sequences, learners can gain a deeper understanding of how the learning rate affects performance, enabling them to grasp the critical nature of parameter selection in machine learning. The objective is to foster an understanding of cause-effect relationships and decision-making in algorithm tuning, allowing learners to apply these concepts effectively in practical contexts.",
    "code": "```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Create the objective function\ndef f(x):\n    return x**2\n\n# Settings for learning rates\nlearning_rates = [0.01, 0.2, 0.5]\ninitial_point = 3\nn_iterations = 10\n\n# Prepare subplots\nfig, axs = plt.subplots(1, 3, figsize=(15, 5))\nx = np.linspace(-3, 3, 100)\ny = f(x)\n\n# Create plots for each learning rate\nfor i, lr in enumerate(learning_rates):\n    x_current = initial_point\n    axs[i].plot(x, y, 'b-', label='Objective Function: f(x)=x^2')\n    \n    # Perform gradient descent\n    for j in range(n_iterations):\n        gradient = 2 * x_current\n        x_next = x_current - lr * gradient\n        axs[i].plot([x_current, x_next], [f(x_current), f(x_next)], 'ro-')\n        x_current = x_next\n    \n    axs[i].set_title(f'Learning Rate (\u03b7) = {lr}')\n    axs[i].set_xlabel('x')\n    axs[i].set_ylabel('f(x)')\n    axs[i].axhline(0, color='black', lw=0.5)\n    axs[i].axvline(0, color='black', lw=0.5)\n    axs[i].set_ylim(0, 10)\n    axs[i].legend()\n    axs[i].grid()\n\nplt.suptitle('Gradient Descent: Impact of Learning Rate on Convergence')\nplt.tight_layout(rect=[0, 0, 1, 0.95])\n```"
}