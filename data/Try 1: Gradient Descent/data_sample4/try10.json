{
    "data": "Gradient Descent Algorithm Gradient Descent Algorithm iteratively calculates the next point using gradient at the current position, scales it (by a learning rate) and subtracts obtained value from the current position (makes a step). It subtracts the value because we want to minimise the function (to maximise it would be adding). This process can be written as:p_{n+1} = p_n - \u03b7 * \u2207f(p_n) There\u2019s an important parameter \u03b7 which scales the gradient and thus controls the step size. In machine learning, it is called learning rate and have a strong influence on performance. The smaller learning rate the longer GD converges, or may reach maximum iteration before reaching the optimum point If learning rate is too big the algorithm may not converge to the optimal point (jump around) or even to diverge completely. In summary, Gradient Descent method\u2019s steps are: 1-choose a starting point (initialisation), 2-calculate gradient at this point, 3-make a scaled step in the opposite direction to the gradient (objective: minimise), 4-repeat points 2 and 3 until one of the criteria is met: maximum number of iterations reached step size is smaller than the tolerance (due to scaling or a small gradient).",
    "goal": "**Principle to be Visualized:** The impact of the learning rate (\u03b7) in the Gradient Descent algorithm and its influence on convergence towards an optimal solution.\n\n**Query:** Create a visualization to explain how varying the learning rate (\u03b7) affects the behavior and convergence of the Gradient Descent algorithm. The goal is to clarify the relationship between the learning rate and the algorithm's ability to minimize a function effectively. The visualization should depict three scenarios: one with a small learning rate (slower convergence), one with an optimal learning rate (efficient convergence), and one with a large learning rate (divergence and oscillation). Using a flowchart style, illustrate each step of the process, including the initial point, gradient calculation, and movement towards the minimum. Highlighting these relationships will deepen learners' understanding of how different learning rates affect convergence rates and outcomes in machine learning tasks.\n\n**Educational Outcome:** The objective is for learners to develop a nuanced understanding of the critical role of the learning rate in the Gradient Descent process. They should be able to analyze how different learning rates can lead to different convergence paths, allowing them to make informed decisions when implementing machine learning algorithms.",
    "code": "```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Function to minimize\ndef f(x):\n    return x**2\n\n# Gradient of the function\ndef gradient(x):\n    return 2*x\n\n# Learning rates\nlearning_rates = [0.05, 0.2, 0.5]\ncolors = ['blue', 'green', 'red']\ntitles = ['Small Learning Rate (\u03b7 = 0.05)', 'Optimal Learning Rate (\u03b7 = 0.2)', 'Large Learning Rate (\u03b7 = 0.5)']\n\n# Initialize subplot\nfig, axs = plt.subplots(1, 3, figsize=(18, 5))\nx = np.linspace(-2, 2, 200)\naxs[0].plot(x, f(x), 'k-', linewidth=2, label='f(x) = x\u00b2')\naxs[1].plot(x, f(x), 'k-', linewidth=2)\naxs[2].plot(x, f(x), 'k-', linewidth=2)\n\nfor i, lr in enumerate(learning_rates):\n    x_current = 1.5\n    iterations = []\n    \n    for _ in range(10):\n        iterations.append(x_current)\n        x_current = x_current - lr * gradient(x_current)\n\n    axs[i].scatter(iterations, f(np.array(iterations)), color='orange', label='Iterations', zorder=5)\n    axs[i].plot(iterations, f(np.array(iterations)), color=colors[i], label='Path', linewidth=2)\n    axs[i].set_title(titles[i])\n    axs[i].set_xlabel('x')\n    axs[i].set_ylabel('f(x)')\n    axs[i].set_ylim(0, 3)\n    axs[i].legend()\n    axs[i].grid()\n\nplt.suptitle('Impact of Learning Rate (\u03b7) on Gradient Descent Convergence', fontsize=16)\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])\n```"
}