{
    "data": "Gradient Descent Algorithm Gradient Descent Algorithm iteratively calculates the next point using gradient at the current position, scales it (by a learning rate) and subtracts obtained value from the current position (makes a step). It subtracts the value because we want to minimise the function (to maximise it would be adding). This process can be written as:p_{n+1} = p_n - \u03b7 * \u2207f(p_n) There\u2019s an important parameter \u03b7 which scales the gradient and thus controls the step size. In machine learning, it is called learning rate and have a strong influence on performance. The smaller learning rate the longer GD converges, or may reach maximum iteration before reaching the optimum point If learning rate is too big the algorithm may not converge to the optimal point (jump around) or even to diverge completely. In summary, Gradient Descent method\u2019s steps are: 1-choose a starting point (initialisation), 2-calculate gradient at this point, 3-make a scaled step in the opposite direction to the gradient (objective: minimise), 4-repeat points 2 and 3 until one of the criteria is met: maximum number of iterations reached step size is smaller than the tolerance (due to scaling or a small gradient).",
    "goal": "**Core Principle:** The effect of learning rate on the convergence and effectiveness of the Gradient Descent algorithm.\n\n**Query:**\nCreate a visualization to explain how different learning rates affect the convergence of the Gradient Descent algorithm. The goal of the visualization is to illustrate the relationship between the learning rate and the efficiency of reaching the optimal point through a series of iterative steps. It should clarify how a small learning rate results in slow convergence, while a large learning rate can cause overshooting or divergence. \n\nThe visualization should include a flowchart that shows the iterative process of Gradient Descent, alongside multiple graphs plotting the trajectory of the algorithm under varying learning rates. Use different colors to represent the paths taken with small, medium, and large learning rates, highlighting their respective outcomes and divergence points. This will aid in understanding by visually demonstrating cause-and-effect relationships between learning rate choices and convergence behavior.\n\n**Educational Objective:**\nThe objective is to ensure that learners deeply understand the critical role of the learning rate in the Gradient Descent process. Students should be able to analyze how different learning rates influence the performance of the algorithm, thereby fostering a practical understanding of optimizing model training in machine learning.",
    "code": "```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the cost function\ndef cost_function(x):\n    return (x - 3) ** 2\n\n# Gradient of the cost function\ndef gradient(x):\n    return 2 * (x - 3)\n\n# Gradient Descent Iterations\ndef gradient_descent(start_x, learning_rate, iterations):\n    x_values = [start_x]\n    for _ in range(iterations):\n        gradient_value = gradient(x_values[-1])\n        next_x = x_values[-1] - learning_rate * gradient_value\n        x_values.append(next_x)\n    return x_values\n\n# Parameters\niterations = 20\nlearning_rates = [0.01, 0.1, 0.5]  # Small, Medium, and Large Learning Rates\ncolors = ['blue', 'orange', 'red']\n\n# Create figure\nfig, ax = plt.subplots(1, 2, figsize=(14, 6))\n\n# Plot Cost Function\nx_range = np.linspace(0, 6, 100)\nax[0].plot(x_range, cost_function(x_range), label='Cost Function', color='gray', linestyle='dashed')\nax[0].set_title('Cost Function')\nax[0].set_xlabel('x')\nax[0].set_ylabel('Cost')\nax[0].grid()\n\n# Collect trajectories for different learning rates\nfor lr, color in zip(learning_rates, colors):\n    trajectory = gradient_descent(start_x=0, learning_rate=lr, iterations=iterations)\n    ax[0].scatter(trajectory, cost_function(np.array(trajectory)), color=color)\n    ax[0].plot(trajectory, cost_function(np.array(trajectory)), color=color, label=f'LR = {lr}')\n\n# Legend for the first plot\nax[0].legend(loc='upper right')\n\n# Flowchart for Gradient Descent Process\nax[1].text(0.5, 0.8, 'Start', ha='center', fontsize=12, bbox=dict(facecolor='lightblue', edgecolor='black'))\nax[1].arrow(0.5, 0.75, 0, -0.1, head_width=0.02, head_length=0.05)\n\nax[1].text(0.5, 0.65, 'Calculate Gradient\\nof Cost Function', ha='center', fontsize=12, bbox=dict(facecolor='lightgreen', edgecolor='black'))\nax[1].arrow(0.5, 0.60, 0, -0.1, head_width=0.02, head_length=0.05)\n\nax[1].text(0.5, 0.50, 'Update Parameters:\\nnew_x = old_x - lr * gradient', ha='center', fontsize=12, bbox=dict(facecolor='lightyellow', edgecolor='black'))\nax[1].arrow(0.5, 0.45, 0, -0.1, head_width=0.02, head_length=0.05)\n\nax[1].text(0.5, 0.35, 'Check Convergence', ha='center', fontsize=12, bbox=dict(facecolor='lightcoral', edgecolor='black'))\nax[1].arrow(0.5, 0.30, 0, -0.1, head_width=0.02, head_length=0.05)\n\nax[1].text(0.5, 0.20, 'If Not Converged\\nRepeat', ha='center', fontsize=12, bbox=dict(facecolor='lightgray', edgecolor='black'))\n\n# Hide axes for the flowchart\nax[1].axis('off')\n\nplt.tight_layout()\n```"
}