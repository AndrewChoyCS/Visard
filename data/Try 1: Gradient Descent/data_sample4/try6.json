{
    "data": "Gradient Descent Algorithm Gradient Descent Algorithm iteratively calculates the next point using gradient at the current position, scales it (by a learning rate) and subtracts obtained value from the current position (makes a step). It subtracts the value because we want to minimise the function (to maximise it would be adding). This process can be written as:p_{n+1} = p_n - \u03b7 * \u2207f(p_n) There\u2019s an important parameter \u03b7 which scales the gradient and thus controls the step size. In machine learning, it is called learning rate and have a strong influence on performance. The smaller learning rate the longer GD converges, or may reach maximum iteration before reaching the optimum point If learning rate is too big the algorithm may not converge to the optimal point (jump around) or even to diverge completely. In summary, Gradient Descent method\u2019s steps are: 1-choose a starting point (initialisation), 2-calculate gradient at this point, 3-make a scaled step in the opposite direction to the gradient (objective: minimise), 4-repeat points 2 and 3 until one of the criteria is met: maximum number of iterations reached step size is smaller than the tolerance (due to scaling or a small gradient).",
    "goal": "**Core Principle: The Impact of Learning Rate on Gradient Descent Convergence**\n\n**Query:** Create a visualization to explain the role of the learning rate (\u03b7) in the Gradient Descent Algorithm and its impact on convergence behavior. The goal is to illustrate how varying learning rates influence the algorithm's steps toward optimization, showing both successful and unsuccessful convergence scenarios. This visualization should clarify how a smaller learning rate results in slower convergence and the potential for reaching a maximum iteration, while a larger learning rate may lead to oscillations or divergence from the optimal point.\n\nThe proposed visualization could be an infographic that includes both graphs and flowcharts. One section should depict a graph demonstrating convergence with a small learning rate versus a larger learning rate, highlighting paths taken to reach the optimum point (or lack thereof). Another section could use a flowchart that delineates the sequential steps of gradient descent, integrating decision points like whether the current learning rate is effective or if adjustments are needed.\n\nThis visual approach aids in understanding the causal relationships between the learning rate settings and the resulting convergence behaviors, deepening learners\u2019 comprehension of how crucially this parameter affects performance in the Gradient Descent Algorithm.\n\n**Objective:** Develop a deep understanding of how the learning rate influences the efficiency and success of the Gradient Descent Algorithm, enabling learners to make informed decisions when selecting learning rates in their own machine learning applications.",
    "code": "```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the quadratic function and its gradient\ndef quadratic_function(x):\n    return (x - 2) ** 2\n\ndef gradient(x):\n    return 2 * (x - 2)\n\n# Parameters\nlearning_rates = [0.01, 0.5]\nmax_iterations = 20\nx_start = 0\n\n# Create Figure\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\nx_values = np.linspace(-1, 5, 100)\n\n# Plotting the quadratic function\nax1.plot(x_values, quadratic_function(x_values), label='f(x) = (x - 2)^2', color='blue')\nax1.axhline(0, color='black', lw=0.5, ls='--')\n\n# Small learning rate\nx_small_lr = x_start\nsmall_lr_path = [x_small_lr]\nfor _ in range(max_iterations):\n    x_small_lr -= learning_rates[0] * gradient(x_small_lr)\n    small_lr_path.append(x_small_lr)\nsmall_lr_path = np.array(small_lr_path)\nax1.plot(small_lr_path, quadratic_function(small_lr_path), \n         marker='o', markersize=5, label='Small LR Path', color='green')\n\n# Large learning rate\nx_large_lr = x_start\nlarge_lr_path = [x_large_lr]\nfor _ in range(max_iterations):\n    x_large_lr -= learning_rates[1] * gradient(x_large_lr)\n    large_lr_path.append(x_large_lr)\nlarge_lr_path = np.array(large_lr_path)\nax1.plot(large_lr_path, quadratic_function(large_lr_path), \n         marker='o', markersize=5, label='Large LR Path', color='red')\n\n# Add titles and labels\nax1.set_title('Impact of Learning Rate on Convergence')\nax1.set_xlabel('x values')\nax1.set_ylabel('f(x)')\nax1.legend()\nax1.grid()\n\n# Flowchart Section\nflowchart_labels = [\n    \"Start\\n(initialize x)\",\n    \"Calculate Gradient\",\n    \"Update: x = x - \u03b7 * Gradient\",\n    \"Check Convergence?\",\n    \"Max Iterations Reached?\",\n    \"Output Result\"\n]\n\n# Decision points\ndecision_points = [\n    \"Are \u03b7 too high?\",\n    \"Is \u03b7 too low?\"\n]\n\n# Prepare positions for flowchart\npositions = {\n    \"Start\": (0, 5),\n    \"Gradient\": (0, 4),\n    \"Update\": (0, 3),\n    \"Convergence\": (0, 2),\n    \"Max Iterations\": (0, 1),\n    \"Output\": (0, 0)\n}\n\n# Add flowchart\nfor label, pos in positions.items():\n    ax2.text(pos[0], pos[1], label, ha='center', backgroundcolor='lightyellow', fontsize=10, bbox=dict(boxstyle=\"round,pad=0.3\", edgecolor='black'))\n\n# Add connections\narrowprops = dict(facecolor='black', arrowstyle='->')\nax2.annotate('', xy=(0, 4.5), xytext=(0, 5), arrowprops=arrowprops)\nax2.annotate('', xy=(0, 3.5), xytext=(0, 4), arrowprops=arrowprops)\nax2.annotate('', xy=(0, 2.5), xytext=(0, 3), arrowprops=arrowprops)\nax2.annotate('', xy=(0, 1.5), xytext=(0, 2), arrowprops=arrowprops)\nax2.annotate('', xy=(0, 0.5), xytext=(0, 1), arrowprops=arrowprops)\n\n# Annotations for decision points\nfor i, point in enumerate(decision_points):\n    ax2.text(2, 2-i, point, ha='center', backgroundcolor='lightgreen', fontsize=10, bbox=dict(boxstyle=\"round,pad=0.3\", edgecolor='black'))\n    if i == 0:  # Are \u03b7 too high?\n        ax2.annotate('', xy=(0, 2), xytext=(1, 2), arrowprops=arrowprops)\n    else:  # Is \u03b7 too low?\n        ax2.annotate('', xy=(0, 1), xytext=(1, 1), arrowprops=arrowprops)\n\n# Finalize flowchart area\nax2.axis('off')\nax2.set_title('Gradient Descent Flowchart')\n\nplt.tight_layout()\n```"
}