{
    "data": "Gradient descent is a method for unconstrained mathematical optimization. It is a first-order iterative algorithm for minimizing a differentiable multivariate function. The idea is to take repeated steps in the opposite direction of the gradient (or approximate gradient) of the function at the current point, because this is the direction of steepest descent. Conversely, stepping in the direction of the gradient will lead to a trajectory that maximizes that function; the procedure is then known as gradient ascent. It is particularly useful in machine learning for minimizing the cost or loss function.",
    "goal": "**Core Principle**: The principle of navigating a multivariate function by using gradient descent and ascent to minimize or maximize a specific objective.\n\n**Query**: Create a visualization to explain the concept of gradient descent and ascent within the context of optimizing a multivariate function. The goal of this visualization is to demonstrate how the position on a multi-dimensional surface changes as we take iterative steps toward minimizing an objective function (gradient descent) and maximizing it (gradient ascent). The visualization should highlight the relationship between the gradient direction, the concept of steepest descent, and effective movement through the function's landscape. \n\nUse a combination of a 3D surface plot to represent the function, arrows to indicate movement directions (both gradient descent and ascent), and annotations to clarify the significance of each step taken. This visual can help learners grasp the cause-and-effect dynamics of how changes in direction affect the function\u2019s output, ultimately leading to a deeper understanding of optimization strategies in machine learning.\n\n**Objective**: To enable learners to precisely understand how iterative movements along the gradient impact the values of a multivariate function, emphasizing the sequential decision-making process involved in optimization techniques.",
    "code": "```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Create the data for the function\nX = np.linspace(-3, 3, 100)\nY = np.linspace(-3, 3, 100)\nX, Y = np.meshgrid(X, Y)\nZ = np.sin(np.sqrt(X**2 + Y**2))\n\n# Plotting the surface\nfig = plt.figure(figsize=(10, 8))\nax = fig.add_subplot(111, projection='3d')\nax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.7)\n\n# Initial points for gradient descent and ascent\nstart_point = np.array([2.5, 1.5])\nend_point_descend = np.array([2, 1])  # After one step of descent\nend_point_ascent = np.array([2.5, 2])  # After one step of ascent\n\n# Calculate gradients\ndef gradient(x, y):\n    dz_dx = np.cos(np.sqrt(x**2 + y**2)) * (x / np.sqrt(x**2 + y**2))\n    dz_dy = np.cos(np.sqrt(x**2 + y**2)) * (y / np.sqrt(x**2 + y**2))\n    return np.array([dz_dx, dz_dy])\n\n# Directions\ngrad_descend = -gradient(start_point[0], start_point[1])\ngrad_ascent = gradient(start_point[0], start_point[1])\n\n# Arrows for movements\nax.quiver(start_point[0], start_point[1], Z[int(start_point[0]+3), int(start_point[1]+3)], \n          end_point_descend[0], end_point_descend[1], Z[int(end_point_descend[0]+3), int(end_point_descend[1]+3)] - Z[int(start_point[0]+3), int(start_point[1]+3)], \n          color='r', linewidth=2, label='Gradient Descent', arrow_length_ratio=0.1)\nax.quiver(start_point[0], start_point[1], Z[int(start_point[0]+3), int(start_point[1]+3)], \n          end_point_ascent[0], end_point_ascent[1], Z[int(end_point_ascent[0]+3), int(end_point_ascent[1]+3)] - Z[int(start_point[0]+3), int(start_point[1]+3)], \n          color='b', linewidth=2, label='Gradient Ascent', arrow_length_ratio=0.1)\n\n# Annotations\nax.text2D(0.05, 0.95, \"Gradient Descent (Red) vs Gradient Ascent (Blue)\", transform=ax.transAxes, fontsize=12)\nax.text(start_point[0], start_point[1], Z[int(start_point[0]+3), int(start_point[1]+3)], \" Start\", color='black')\nax.text(end_point_descend[0], end_point_descend[1], Z[int(end_point_descend[0]+3), int(end_point_descend[1]+3)], \" Descent\", color='red')\nax.text(end_point_ascent[0], end_point_ascent[1], Z[int(end_point_ascent[0]+3), int(end_point_ascent[1]+3)], \" Ascent\", color='blue')\n\n# Labels and titles\nax.set_xlabel('X-axis')\nax.set_ylabel('Y-axis')\nax.set_zlabel('Objective Function Value')\nax.set_title('Gradient Descent and Ascent on a Multivariate Function')\nax.legend()\n```"
}