{
    "data": "Gradient descent is a technique used for unconstrained optimization in mathematics. It is an iterative algorithm of the first order, utilized for lowering the value of differentiable multivariable functions. The fundamental idea involves taking consecutive steps in the direction opposite to the gradient (or an approximate version of it) found at the present location, as this route corresponds to the steepest decline. On the other hand, moving in the same direction as the gradient results in a trail that maximizes the function, a method referred to as gradient ascent. This approach is extremely valuable in the realm of machine learning for reducing the cost or loss associated with a function.",
    "goal": "**Extracted Core Principle: The Direction of Gradient Descent and Gradient Ascent in Optimization**\n\n**Query:**\nCreate a visualization to explain the concepts of gradient descent and gradient ascent in the context of optimization for differentiable multivariable functions. The goal is to illustrate the relationship between the gradient direction and the resulting paths of optimization\u2014one leading to a minimum (gradient descent) and the other to a maximum (gradient ascent). This visualization should clarify how taking steps opposite to the gradient leads to decreased function values, while moving with the gradient yields increased values. \n\nUtilize a combination of graphs and diagrams to depict the function landscape, the gradient as a vector, and the iterations of both gradient descent and ascent. These visual elements will help learners grasp the cause-and-effect relationships inherent in optimization processes and highlight the sequential nature of reaching extrema in a multidimensional space.\n\n**Educational Outcomes:**\nParticipants will achieve a deep understanding of how gradient descent and ascent operate, recognize the significance of the gradient\u2019s direction, and appreciate its application in machine learning contexts. By visually engaging with the material, learners will develop a conceptual framework that demonstrates the iterative nature of the optimization process, reinforcing their comprehension of both techniques and their roles in minimizing and maximizing functions.",
    "code": "```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\nx = np.linspace(-3, 3, 100)\ny = np.linspace(-3, 3, 100)\nX, Y = np.meshgrid(x, y)\nZ = np.sin(np.sqrt(X**2 + Y**2))\n\nfig = plt.figure(figsize=(14, 6))\n\n# 3D function surface\nax1 = fig.add_subplot(121, projection='3d')\nax1.plot_surface(X, Y, Z, cmap='viridis', alpha=0.6)\nax1.set_title('Function Landscape')\nax1.set_xlabel('X-axis')\nax1.set_ylabel('Y-axis')\nax1.set_zlabel('Z-axis')\nax1.view_init(30, 210)\n\n# Gradients\ngrad_x, grad_y = np.gradient(Z)\nstart = (-2.5, -2.5)\nax1.quiver(start[0], start[1], np.sin(np.sqrt(start[0]**2 + start[1]**2)), \n            -grad_x[50, 50], -grad_y[50, 50], 0, color='r', length=0.5, label='Gradient Descent')\nax1.quiver(start[0], start[1], np.sin(np.sqrt(start[0]**2 + start[1]**2)), \n            grad_x[50, 50], grad_y[50, 50], 0, color='b', length=0.5, label='Gradient Ascent')\n\n# Iteration paths\ndescent_path = [(-2.5, -2.5)]\nascent_path = [(-2.5, -2.5)]\n\nfor _ in range(10):\n    current = descent_path[-1]\n    grad = np.array([grad_x[int(current[0]*10)+30, int(current[1]*10)+30], \n                       grad_y[int(current[0]*10)+30, int(current[1]*10)+30]])\n    next_step = current - 0.1 * grad / np.linalg.norm(grad)\n    descent_path.append(next_step)\n\nfor _ in range(10):\n    current = ascent_path[-1]\n    grad = np.array([grad_x[int(current[0]*10)+30, int(current[1]*10)+30], \n                       grad_y[int(current[0]*10)+30, int(current[1]*10)+30]])\n    next_step = current + 0.1 * grad / np.linalg.norm(grad)\n    ascent_path.append(next_step)\n\ndescent_path = np.array(descent_path)\nascent_path = np.array(ascent_path)\n\n# Plot iterations\nax1.plot(descent_path[:, 0], descent_path[:, 1], \n         np.sin(np.sqrt(descent_path[:, 0]**2 + descent_path[:, 1]**2)), color='r', marker='o', label='Descent Path')\nax1.plot(ascent_path[:, 0], ascent_path[:, 1], \n         np.sin(np.sqrt(ascent_path[:, 0]**2 + ascent_path[:, 1]**2)), color='b', marker='o', label='Ascent Path')\n\nax1.legend()\n\n# 2D contour plot\nax2 = fig.add_subplot(122)\nCS = ax2.contourf(X, Y, Z, cmap='viridis', levels=20, alpha=0.6)\nax2.set_title('2D Projection with Optimization Paths')\nax2.set_xlabel('X-axis')\nax2.set_ylabel('Y-axis')\n\n# Paths in contour plot\nax2.plot(descent_path[:, 0], descent_path[:, 1], color='r', marker='o', label='Gradient Descent Path')\nax2.plot(ascent_path[:, 0], ascent_path[:, 1], color='b', marker='o', label='Gradient Ascent Path')\nax2.legend()\nplt.colorbar(CS, ax=ax2, shrink=0.5, aspect=5)\n```"
}