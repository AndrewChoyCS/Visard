{
    "data": "Gradient descent is an optimization technique used for problems without constraints. It employs a first-order iterative approach to reduce the value of a differentiable multivariable function. The central concept is to incrementally move in the direction opposite to the gradient (or its estimation) at the current location, as this path represents the direction of greatest decline. Conversely, if moves are made following the gradient's direction, the function\u2019s value increases and this process is referred to as gradient ascent. This method is especially valuable in the field of machine learning for the reduction of cost or loss functions.",
    "goal": "**Extracted Core Principle:** The direction of movement in gradient descent (opposite to the gradient) is critical for reducing the value of a function, while moving in the same direction as the gradient leads to an increase in the function's value (gradient ascent).\n\n**Query:**\nCreate a visualization to explain the core concept of gradient descent and gradient ascent in optimization algorithms. The goal is to clearly illustrate how moving against the gradient results in the minimization of a differentiable multivariable function, while moving with the gradient leads to maximization. The visualization should include a 3D surface plot representing the function, with arrows depicting the direction of the gradient and contrasting paths emphasizing both descent and ascent. Using a combination of an infographic style with clear annotations will help learners see the relationships between function values and gradient directions, assisting in grasping the iterative nature of the descent process.\n\nThis visual should focus on cause-effect relationships by highlighting how the direction of movement influences the function's value, reinforcing the concept's application in machine learning. By integrating these elements, learners will gain a deeper and more intuitive understanding of gradient descent and its significance. \n\n**Learning Objective:** Learners will understand the fundamental principle of optimization in gradient descent vs. ascent; they will be able to discern how directional movement associated with gradients influences the outcome of multivariable functions, establishing a foundation for further applications in machine learning.",
    "code": "```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Create a grid of points\nx = np.linspace(-3, 3, 100)\ny = np.linspace(-3, 3, 100)\nX, Y = np.meshgrid(x, y)\n\n# Define the multivariable function\nZ = X**2 + Y**2  # A simple convex function\n\n# Calculate the gradients\ndx, dy = np.gradient(Z)\n\n# Create a new figure\nfig = plt.figure(figsize=(10, 8))\nax = fig.add_subplot(111, projection='3d')\n\n# Plot the surface\nax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.7)\n\n# Plot the gradient vectors\nax.quiver(X, Y, Z, -dx, -dy, 0, color='r', length=0.3, arrow_length_ratio=0.1)\n\n# Paths for gradient descent and ascent\ndescent_path = np.array([[2, 2], [1, 1], [0, 0]])\nascent_path = np.array([[-2, -2], [-1, -1], [0, 0]])\n\n# Plot trajectory for descent\nax.plot(descent_path[:, 0], descent_path[:, 1], \n        X[2, 2]-((descent_path[:, 0]**2 + descent_path[:, 1]**2)), \n        marker='o', color='b', label='Gradient Descent Path')\n\n# Plot trajectory for ascent\nax.plot(ascent_path[:, 0], ascent_path[:, 1], \n        X[-2, -2]+((ascent_path[:, 0]**2 + ascent_path[:, 1]**2)), \n        marker='o', color='g', label='Gradient Ascent Path')\n\n# Labels\nax.set_title('Gradient Descent vs Gradient Ascent', fontsize=16)\nax.set_xlabel('X-axis', fontsize=12)\nax.set_ylabel('Y-axis', fontsize=12)\nax.set_zlabel('Function Value', fontsize=12)\nax.legend()\n\n# Annotations\nax.text(2, 2, 4, \"Gradient Descent\\n(Minimization)\", color='blue', fontsize=10)\nax.text(-2, -2, 4, \"Gradient Ascent\\n(Maximization)\", color='green', fontsize=10)\n\nplt.tight_layout()\n```"
}