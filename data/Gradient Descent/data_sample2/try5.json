{
    "data": "The loss functions for linear models always produce a convex surface. As a result of this property, when a linear regression model converges, we know the model has found the weights and bias that produce the lowest loss.If we graph the loss surface for a model with one feature, we can see its convex shape. The following is the loss surface of the miles per gallon dataset used in the previous examples. Weight is on the x-axis, bias is on the y-axis, and loss is on the z-axis:",
    "goal": "**Core Principle:**\nThe convex nature of loss functions in linear models guarantees that the convergence of the linear regression model leads to the optimal weights and bias that minimize loss.\n\n**Query:**\nCreate a visualization to explain the concept of the convex loss surface in linear regression models. The visualization should depict a three-dimensional graph where the x-axis represents weight, the y-axis represents bias, and the z-axis represents loss. The graph should illustrate the convex shape of the loss surface, highlighting the minimum point as the optimal solution. This visualization is intended to clarify the relationship between the weights, bias, and loss, emphasizing how the convexity ensures that convergence leads to the minimum loss. By incorporating interactive elements, such as sliders to adjust weight and bias, learners can gain a deeper understanding of how adjustments affect loss, thus reinforcing the core educational outcome of recognizing the importance of convexity in finding optimal solutions in regression analysis.\n\n**Objective:**\nDevelop a profound understanding of how the convex nature of loss functions ensures that linear regression models can reliably converge to the optimal combination of weights and bias, minimizing loss.",
    "code": "```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib.widgets import Slider\n\n# Create a grid of weights and biases\nweight = np.linspace(-5, 5, 100)\nbias = np.linspace(-5, 5, 100)\nW, B = np.meshgrid(weight, bias)\n\n# Define the loss function (Mean Squared Error)\ndef loss_function(W, B):\n    return (W**2 + B**2)\n\n# Calculate loss\nL = loss_function(W, B)\n\n# Create the figure and 3D axes\nfig = plt.figure(figsize=(10, 7))\nax = fig.add_subplot(111, projection='3d')\n\n# Plot the surface\nsurf = ax.plot_surface(W, B, L, cmap='viridis', alpha=0.8)\n\n# Highlight the minimum point\nmin_loss = np.min(L)\nmin_index = np.where(L == min_loss)\noptimal_weight = W[min_index][0]\noptimal_bias = B[min_index][0]\nax.scatter(optimal_weight, optimal_bias, min_loss, color='red', s=100, label='Optimal Solution')\n\n# Label the axes\nax.set_xlabel('Weight')\nax.set_ylabel('Bias')\nax.set_zlabel('Loss')\nax.set_title('Convex Loss Surface in Linear Regression')\nax.legend()\n\n# Set up sliders for interactive weight and bias adjustment\nax_weight = plt.axes([0.15, 0.01, 0.65, 0.03])\nax_bias = plt.axes([0.15, 0.06, 0.65, 0.03])\nslider_weight = Slider(ax_weight, 'Weight', -5.0, 5.0, valinit=0)\nslider_bias = Slider(ax_bias, 'Bias', -5.0, 5.0, valinit=0)\n\n# Update function for sliders\ndef update(val):\n    weight_val = slider_weight.val\n    bias_val = slider_bias.val\n    loss_val = loss_function(weight_val, bias_val)\n    \n    ax.scatter(weight_val, bias_val, loss_val, color='blue', s=100)\n\nslider_weight.on_changed(update)\nslider_bias.on_changed(update)\n\nplt.colorbar(surf, ax=ax)\n```"
}