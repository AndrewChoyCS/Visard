{
    "data": "Gradient descent is an optimization technique used for problems without constraints. It employs a first-order iterative approach to reduce the value of a differentiable multivariable function. The central concept is to incrementally move in the direction opposite to the gradient (or its estimation) at the current location, as this path represents the direction of greatest decline. Conversely, if moves are made following the gradient's direction, the function\u2019s value increases and this process is referred to as gradient ascent. This method is especially valuable in the field of machine learning for the reduction of cost or loss functions.",
    "goal": "**Core Principle Extracted:** The process of moving in the direction of the gradient vs. moving against the gradient in gradient descent and gradient ascent.\n\n**Query:** Create a visualization to explain the concept of gradient descent and gradient ascent in the context of optimizing a multivariable function. The visualization should clearly differentiate between moving against the gradient (gradient descent) and moving with the gradient (gradient ascent). Specifically, it should illustrate the relationship between the function value and the direction of movement as well as how these movements affect the optimization process. Suggested styles include a dual-line graph showing both processes overlaid on a 3D surface plot of the multivariable function. This will help learners understand the idea of optimization by visually demonstrating how different directions of movement impact the function's value. \n\n**Objective:** The objective is to develop a deep and crucial understanding of how gradient descent minimizes function values while gradient ascent maximizes them, enabling learners to recognize the practical applications of these concepts in machine learning and optimization tasks.",
    "code": "```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Create a meshgrid for the function\nx = np.linspace(-2, 2, 100)\ny = np.linspace(-2, 2, 100)\nX, Y = np.meshgrid(x, y)\n\n# Define the multivariable function Z = f(X, Y)\nZ = -(X**2 + Y**2) + 2  # Concave function for demonstration\n\n# Compute the gradient\ndef gradient(X, Y):\n    dZdx = -2 * X  # Gradient w.r.t. X\n    dZdy = -2 * Y  # Gradient w.r.t. Y\n    return dZdx, dZdy\n\n# Calculate the gradient at a point\nstart_point = np.array([1.5, 1.5])\ngrad = gradient(*start_point)\n\n# Define steps for descent and ascent\nstep_size = 0.1\ndescent_steps = [start_point]\nascent_steps = [start_point]\n\n# Perform gradient descent\nfor _ in range(10):\n    start_point = start_point - step_size * grad\n    descent_steps.append(start_point)\n\n# Reset start point for ascent\nstart_point = np.array([1.5, 1.5])\n\n# Perform gradient ascent\nfor _ in range(10):\n    start_point = start_point + step_size * grad\n    ascent_steps.append(start_point)\n\n# Convert the lists to numpy arrays for plotting\ndescent_steps = np.array(descent_steps)\nascent_steps = np.array(ascent_steps)\n\n# Create the figure\nfig = plt.figure(figsize=(15, 10))\n\n# 3D Surface Plot of the Function\nax = fig.add_subplot(121, projection='3d')\nax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.7)\nax.set_title('Multivariable Function Surface')\nax.set_xlabel('X-axis')\nax.set_ylabel('Y-axis')\nax.set_zlabel('Function Value')\n\n# Plot Gradient Descent\nax.plot(descent_steps[:, 0], descent_steps[:, 1], -((descent_steps[:, 0]**2) + (descent_steps[:, 1]**2)) + 2, marker='o', color='red', label='Gradient Descent', alpha=1)\nax.text(descent_steps[-1][0], descent_steps[-1][1], -((descent_steps[-1][0]**2) + (descent_steps[-1][1]**2)) + 2, \"Local Minima\", color='red')\n\n# 3D Surface Plot of the Function\nax2 = fig.add_subplot(122, projection='3d')\nax2.plot_surface(X, Y, Z, cmap='plasma', alpha=0.7)\nax2.set_title('Multivariable Function Surface')\nax2.set_xlabel('X-axis')\nax2.set_ylabel('Y-axis')\nax2.set_zlabel('Function Value')\n\n# Plot Gradient Ascent\nax2.plot(ascent_steps[:, 0], ascent_steps[:, 1], -((ascent_steps[:, 0]**2) + (ascent_steps[:, 1]**2)) + 2, marker='o', color='blue', label='Gradient Ascent', alpha=1)\nax2.text(ascent_steps[-1][0], ascent_steps[-1][1], -((ascent_steps[-1][0]**2) + (ascent_steps[-1][1]**2)) + 2, \"Local Maxima\", color='blue')\n\n# Legends\nax.legend()\nax2.legend()\nplt.tight_layout()\n```"
}