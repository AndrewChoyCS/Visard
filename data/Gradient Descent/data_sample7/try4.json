{
    "data": "Gradient descent is an approach utilized for unconstrained mathematical optimization. This first-order iterative method aims to minimize a multivariate function that is differentiable. The technique involves taking consistent steps in the reverse direction of the gradient\u2014or an estimated gradient\u2014at the current position, as this points towards the pathway of the greatest decline. On the other hand, making progress in the direction of the gradient will yield a path that maximizes the function, a method referred to as gradient ascent. This strategy is especially valuable in machine learning for reducing the cost or loss function.",
    "goal": "**Core Principle:** The relationship between gradient descent and gradient ascent in optimizing multivariate functions through directional movement along the gradient.\n\n**Query:** Create a visualization to explain the concepts of gradient descent and gradient ascent in the context of mathematical optimization. The goal of the visualization is to clearly illustrate how moving in the direction of the gradient increases function value (gradient ascent), while moving in the opposite direction decreases function value (gradient descent). This will help clarify the duality of these optimization techniques and their applications in machine learning for minimizing cost or loss functions. \n\nThe visualization should include an infographic that juxtaposes the two methods, highlighting the directional flow along the gradients on a 3D surface graph representing the multivariate function. Arrows should represent the direction of movement (with distinct color coding for ascent vs. descent), and gradient magnitude could be depicted through varying arrow thickness to indicate steepness. This illustration will enhance understanding of the cause-effect relationships inherent in optimization strategies, emphasizing how the iterative aspect of these methods leads to convergence on optimal values.  \n\n**Learning Objective:** Students will be able to clearly differentiate and articulate the roles of gradient descent and ascent in optimization, recognizing the significance of directional movement along gradients in minimizing or maximizing functions, thereby deepening their understanding of functional optimization and its real-world applications in machine learning.",
    "code": "```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Create a 3D grid\nX = np.linspace(-3, 3, 100)\nY = np.linspace(-3, 3, 100)\nX, Y = np.meshgrid(X, Y)\nZ = -X**2 - Y**2 + 9 # Multivariate function for maxima (gradient ascent)\nZ_min = X**2 + Y**2  # Multivariate function for minima (gradient descent)\n\n# Compute gradients\ngrad_ascent = np.array(np.gradient(Z))\ngrad_descent = np.array(np.gradient(Z_min))\n\n# Setup a 3D plot\nfig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(111, projection='3d')\n\n# Plot gradient ascent\nax.plot_surface(X, Y, Z, alpha=0.5, cmap='viridis', edgecolor='none')\n\n# Plot gradient descent\nax.plot_surface(X, Y, Z_min, alpha=0.5, cmap='plasma', edgecolor='none')\n\n# Arrows for Gradient Ascent\nN = 10\nfor i in range(0, len(X), N):\n    for j in range(0, len(Y), N):\n        ax.quiver(X[i, j], Y[i, j], Z[i, j], 0.5*grad_ascent[1][i, j], 0.5*grad_ascent[0][i, j], 0,\n                   color='blue', label='Gradient Ascent' if (i == 0 and j == 0) else \"\", \n                   arrow_length_ratio=0.1, linewidth=2.5)\n\n# Arrows for Gradient Descent\nfor i in range(0, len(X), N):\n    for j in range(0, len(Y), N):\n        ax.quiver(X[i, j], Y[i, j], Z_min[i, j], -0.5*grad_descent[1][i, j], -0.5*grad_descent[0][i, j], 0,\n                   color='red', label='Gradient Descent' if (i == 0 and j == 0) else \"\", \n                   arrow_length_ratio=0.1, linewidth=2.5)\n\n# Setting labels\nax.set_title('Gradient Descent vs. Gradient Ascent', fontsize=20)\nax.set_xlabel('X-axis', fontsize=15)\nax.set_ylabel('Y-axis', fontsize=15)\nax.set_zlabel('Function Value', fontsize=15)\nax.legend(loc='upper left')\n\n# Adjust view angle\nax.view_init(elev=25, azim=30)\n```"
}