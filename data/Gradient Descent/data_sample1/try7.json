{
    "data": "Gradient descent is a method for unconstrained mathematical optimization. It is a first-order iterative algorithm for minimizing a differentiable multivariate function. The idea is to take repeated steps in the opposite direction of the gradient (or approximate gradient) of the function at the current point, because this is the direction of steepest descent. Conversely, stepping in the direction of the gradient will lead to a trajectory that maximizes that function; the procedure is then known as gradient ascent. It is particularly useful in machine learning for minimizing the cost or loss function.",
    "goal": "**Core Principle:** The process of gradient descent, including its directionality and relationship to gradient ascent, as a method for optimizing differentiable multivariate functions.\n\n**Query:** Create a visualization to explain the concept of gradient descent as an optimization technique in the context of minimizing a differentiable multivariate function. The goal of the visualization is to clarify how repeated steps in the direction opposite to the gradient lead to decreased function values (steepest descent), while steps in the direction of the gradient lead to increased values (gradient ascent). The expected educational outcomes include a deep understanding of the iterative process of gradient descent versus gradient ascent, as well as the practical implications of this technique in machine learning for minimizing cost or loss functions. \n\nFor the visualization style, consider using a **flowchart combined with a gradient descent graph**, displaying both descending steps towards a minimum and ascending steps towards a maximum, clearly labeling axes, gradients, and function values. This dual representation illustrates the relationships and sequential processes involved, helping learners visualize the outcomes of different directional choices (descent vs. ascent) in a tangible way.\n\n**Objective:** By engaging with this visualization, learners will be able to articulate the mechanics and implications of gradient descent and ascent, fostering a deeper understanding of their roles in optimization and machine learning applications.",
    "code": "```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Data for the surface\nx = np.linspace(-3, 3, 100)\ny = np.linspace(-3, 3, 100)\nX, Y = np.meshgrid(x, y)\nZ = X**2 + Y**2  # Example function: f(x, y) = x^2 + y^2\n\n# Gradient calculation (gradient of f)\ndef gradient(X, Y):\n    dX = 2 * X  # Partial derivative with respect to X\n    dY = 2 * Y  # Partial derivative with respect to Y\n    return np.array([dX, dY])\n\n# Create figure\nfig = plt.figure(figsize=(14, 8))\nax = fig.add_subplot(111, projection='3d')\n\n# Plot the surface\nax.plot_surface(X, Y, Z, alpha=0.5, rstride=100, cstride=100, color='lightblue')\n\n# Gradient descent path\nstart = np.array([2.5, 2.5])  # Starting point\nlearning_rate = 0.1\nsteps = 10\npoints = [start]\n\nfor i in range(steps):\n    grad = gradient(points[-1][0], points[-1][1])\n    next_point = points[-1] - learning_rate * grad / np.linalg.norm(grad)\n    points.append(next_point)\n\npoints = np.array(points)\nax.plot(points[:, 0], points[:, 1], Z[points[:, 0].astype(int), points[:, 1].astype(int)], marker='o', color='red', label='Gradient Descent Path')\n\n# Gradient ascent path\nstart_ascent = np.array([-2.5, -2.5])  # Starting point for ascent\npoints_ascent = [start_ascent]\n\nfor i in range(steps):\n    grad = gradient(points_ascent[-1][0], points_ascent[-1][1])\n    next_point_ascent = points_ascent[-1] + learning_rate * grad / np.linalg.norm(grad)\n    points_ascent.append(next_point_ascent)\n\npoints_ascent = np.array(points_ascent)\nax.plot(points_ascent[:, 0], points_ascent[:, 1], Z[points_ascent[:, 0].astype(int), points_ascent[:, 1].astype(int)], marker='o', color='green', label='Gradient Ascent Path')\n\n# Labels and legend\nax.set_xlabel('X-axis')\nax.set_ylabel('Y-axis')\nax.set_zlabel('Function Value')\nax.set_title('Gradient Descent vs Gradient Ascent')\nax.legend()\n\n# Show gradient directions\nfor point in points:\n    ax.quiver(point[0], point[1], Z[point[0].astype(int), point[1].astype(int)], -2*point[0], -2*point[1], -1, color='red', arrow_length_ratio=0.1)\n\nfor point in points_ascent:\n    ax.quiver(point[0], point[1], Z[point[0].astype(int), point[1].astype(int)], 2*point[0], 2*point[1], 1, color='green', arrow_length_ratio=0.1)\n\n# Adjust view angle\nax.view_init(elev=30, azim=30)\n\nplt.grid()\n```"
}