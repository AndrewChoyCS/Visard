{
    "data": "Gradient descent is a method for unconstrained mathematical optimization. It is a first-order iterative algorithm for minimizing a differentiable multivariate function. The idea is to take repeated steps in the opposite direction of the gradient (or approximate gradient) of the function at the current point, because this is the direction of steepest descent. Conversely, stepping in the direction of the gradient will lead to a trajectory that maximizes that function; the procedure is then known as gradient ascent. It is particularly useful in machine learning for minimizing the cost or loss function.",
    "goal": "**Core Principle:** The concept of gradient descent as an iterative method of optimization that involves taking steps in the opposite direction of the gradient to minimize a function.\n\n---\n\n**Query:** Create a visualization to explain the iterative process of gradient descent, illustrating the concept of moving in the direction of the steepest descent to minimize a multivariate function. The visualization should depict a 3D graph where the terrain represents the function being minimized, showcasing the gradient vectors and the trajectory of steps taken in both the direction of the gradient (to signify gradient ascent) and the opposite direction (for gradient descent). Highlight the cost or loss function values at different points along the trajectory. \n\nThe goal is to clarify the relationship between the gradient, the direction of movement, and the resulting change in function values, helping learners understand how each step brings them closer to the minimum. By integrating best practices such as clear labeling, color-coded paths for descent and ascent, and annotations for critical points, the visualization should deepen the learner\u2019s insight into the cause-and-effect relationship of the optimization process. \n\n**Learning Objective:** Students will gain a profound understanding of how gradient descent iteratively approaches the minimum of a function by visualizing the relationship between gradients and the optimization trajectory, enhancing their ability to apply this principle in machine learning contexts.",
    "code": "```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Create grid data\nx = np.linspace(-3, 3, 100)\ny = np.linspace(-3, 3, 100)\nX, Y = np.meshgrid(x, y)\nZ = np.sin(np.sqrt(X**2 + Y**2))  # Function to minimize\n\n# Gradient function\ndef gradient(X, Y):\n    dZdx = np.cos(np.sqrt(X**2 + Y**2)) * (X / np.sqrt(X**2 + Y**2))\n    dZdy = np.cos(np.sqrt(X**2 + Y**2)) * (Y / np.sqrt(X**2 + Y**2))\n    return np.array([dZdx, dZdy])\n\n# Initial point\npoint = np.array([-2.5, -2.5])\nlearning_rate = 0.1\n\n# Arrays to store trajectory points and loss values\ntrajectory = [point.copy()]\nloss_values = [np.sin(np.sqrt(point[0]**2 + point[1]**2))]\n\n# Iteratively move towards the minimum\nfor _ in range(20):\n    grad = gradient(point[0], point[1])\n    point = point - learning_rate * grad.flatten()  # Gradient Descent\n    trajectory.append(point.copy())\n    loss_values.append(np.sin(np.sqrt(point[0]**2 + point[1]**2)))\n\ntrajectory = np.array(trajectory)\n\n# Creating 3D plot\nfig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(111, projection='3d')\n\n# Surface plot\nax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.6, rstride=100, cstride=100)\n\n# Gradient ascent (dashed line)\nascent_points = trajectory.copy()\nascent_points[:, 0] += 0.5 * gradient(ascent_points[:, 0], ascent_points[:, 1])[0]\nascent_points[:, 1] += 0.5 * gradient(ascent_points[:, 0], ascent_points[:, 1])[1]\nax.plot(ascent_points[:, 0], ascent_points[:, 1], np.sin(np.sqrt(ascent_points[:, 0]**2 + ascent_points[:, 1]**2)),\n         color='red', linewidth=2, label='Gradient Ascent', linestyle='--')\n\n# Gradient descent trajectory\nax.plot(trajectory[:, 0], trajectory[:, 1], loss_values, color='blue', linewidth=3, label='Gradient Descent', marker='o')\n\n# Annotations and labels\nfor i, point in enumerate(trajectory):\n    ax.text(point[0], point[1], loss_values[i], f'{loss_values[i]:.2f}', color='black')\n\nax.set_title('Gradient Descent Visualization')\nax.set_xlabel('X-axis')\nax.set_ylabel('Y-axis')\nax.set_zlabel('Function Value')\nax.legend()\nax.view_init(elev=30, azim=210)\n```"
}