{
    "data": "Gradient descent is a technique used for unconstrained optimization in mathematics. It is an iterative algorithm of the first order, utilized for lowering the value of differentiable multivariable functions. The fundamental idea involves taking consecutive steps in the direction opposite to the gradient (or an approximate version of it) found at the present location, as this route corresponds to the steepest decline. On the other hand, moving in the same direction as the gradient results in a trail that maximizes the function, a method referred to as gradient ascent. This approach is extremely valuable in the realm of machine learning for reducing the cost or loss associated with a function.",
    "goal": "**Extracted Core Principle:** The process of gradient descent as a means of finding the minimum value of a differentiable multivariable function by iteratively moving in the opposite direction of the gradient.\n\n**Query:** Create a visualization to explain the concept of gradient descent in optimization. The goal of this visualization is to clarify how gradient descent iteratively finds the minimum of a function by illustrating the relationship between the gradient and the direction of movement in optimization. The key educational outcomes expected include a deeper understanding of how successive steps lead to the steepest decline and the contrast between gradient descent and gradient ascent. \n\nThe visualization should utilize a diagram that shows a 3D surface of a multivariable function with arrows indicating the gradient at various points and the subsequent steps taken in the direction of the steepest descent. Additionally, a side panel could show a comparative visualization of gradient ascent to highlight the differences in objectives. By illustrating the stepwise progression and the interaction between the function's shape and the gradient, learners will gain a clearer insight into the mechanics of the algorithm, thereby enriching their conceptual grasp of optimization techniques in machine learning.\n\n**Objective:** To enable learners to comprehend the iterative nature of gradient descent and its significance in unconstrained optimization, allowing them to visualize how each step leads to a local minimum by effectively interpreting gradients and correctly navigating the function's landscape.",
    "code": "```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Create a meshgrid for the surface\nx = np.linspace(-3, 3, 100)\ny = np.linspace(-3, 3, 100)\nX, Y = np.meshgrid(x, y)\nZ = X**2 + Y**2  # Example of a simple quadratic function\n\n# Compute the gradient\ndef gradient(X, Y):\n    dZdx = 2 * X\n    dZdy = 2 * Y\n    return np.array([dZdx, dZdy])\n\n# Initial point\ninitial_point = np.array([-2, 2])\nlearning_rate = 0.1\npoints = [initial_point]\n\n# Iteratively calculate gradient descent steps\nfor _ in range(10):\n    grad = gradient(initial_point[0], initial_point[1])\n    initial_point = initial_point - learning_rate * grad\n    points.append(initial_point)\n\npoints = np.array(points)\n\n# Setup the 3D plot\nfig = plt.figure(figsize=(14, 8))\nax = fig.add_subplot(121, projection='3d')\nax.plot_surface(X, Y, Z, alpha=0.5, rstride=100, cstride=100, color='lightblue')\nax.set_title('Gradient Descent Visualization')\nax.set_xlabel('X axis')\nax.set_ylabel('Y axis')\nax.set_zlabel('Z axis')\n\n# Plot gradient and descent steps\nfor i in range(len(points) - 1):\n    ax.quiver(points[i, 0], points[i, 1], gradient(points[i, 0], points[i, 1])[0][0], \n             gradient(points[i, 0], points[i, 1])[1][0], 0, length=0.5, color='r', arrow_length_ratio=0.2)\n    \n    if i < len(points) - 1:\n        ax.plot([points[i, 0], points[i + 1, 0]], \n                [points[i, 1], points[i + 1, 1]], \n                [Z[int(points[i, 0]+3), int(points[i, 1]+3)], \n                 Z[int(points[i + 1, 0]+3), int(points[i + 1, 1]+3)]], 'go-')\n\n# Create side panel for Gradient Ascent\nax2 = fig.add_subplot(122, projection='3d')\nax2.plot_surface(X, Y, Z, alpha=0.5, rstride=100, cstride=100, color='lightgreen')\nax2.set_title('Gradient Ascent Visualization')\nax2.set_xlabel('X axis')\nax2.set_ylabel('Y axis')\nax2.set_zlabel('Z axis')\n\n# Gradient ascent steps\ninitial_point_ascent = np.array([-2, 2])\npoints_ascent = [initial_point_ascent]\n\nfor _ in range(10):\n    grad = gradient(initial_point_ascent[0], initial_point_ascent[1])\n    initial_point_ascent = initial_point_ascent + learning_rate * grad\n    points_ascent.append(initial_point_ascent)\n\npoints_ascent = np.array(points_ascent)\n\nfor i in range(len(points_ascent) - 1):\n    ax2.quiver(points_ascent[i, 0], points_ascent[i, 1], gradient(points_ascent[i, 0], points_ascent[i, 1])[0][0], \n             gradient(points_ascent[i, 0], points_ascent[i, 1])[1][0], 0, length=0.5, color='b', arrow_length_ratio=0.2)\n    \n    if i < len(points_ascent) - 1:\n        ax2.plot([points_ascent[i, 0], points_ascent[i + 1, 0]], \n                  [points_ascent[i, 1], points_ascent[i + 1, 1]], \n                  [Z[int(points_ascent[i, 0]+3), int(points_ascent[i, 1]+3)], \n                   Z[int(points_ascent[i + 1, 0]+3), int(points_ascent[i + 1, 1]+3)]], 'bo-')\n```"
}